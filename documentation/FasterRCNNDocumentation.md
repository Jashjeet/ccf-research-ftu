# Faster R-CNN Documentation

## Requirements

* pillow
* lxml
* jupyter
* matplotlib
* tensorflow version 1.15
* Tensorflow models for v1.15 (https://github.com/tensorflow/models)
* COCO API (https://github.com/cocodataset/cocoapi.git)
* Proto Buff (protoc-3.11.4-win64.zip from https://github.com/protocolbuffers/protobuf/releases)
* labelImg (https://tzutalin.github.io/labelImg/)

## Data

### Kidney Raw Data
This work utilized a dataset of human kidney whole slide images (WSIs) provided by the HuBMAP Consortium, which is now available at [the HuBMAP Data Portal](https://portal.hubmapconsortium.org/search?origin_sample.mapped_organ[0]=Kidney%20%28Left%29&origin_sample.mapped_organ[1]=Kidney%20%28Right%29&group_name[0]=Vanderbilt%20TMC&entity_type[0]=Dataset). The raw kidney image data was generated by the BIOmolecular Multimodal Imaging Center (BIOMIC) at Vanderbilt University. (See [BIOMIC's page](https://medschool.vanderbilt.edu/biomic/) for more information.) This dataset is composed of 13 samples, each with multiple images that correspond to different microscopy types. All samples include PAS  stain images, and many include imaging mass spectrometry as well. In addition, this dataset provides information on where the sample was positioned within the kidney and whether it was taken from a left or right kidney, which is vital information for the HuBMAP objective.

[Kidney Raw Data](https://drive.google.com/drive/folders/14aLxPR9LlzdWXPomAX1moqL0UnRm_RbW?usp=sharing)

## Algorithm

The Faster R-CNN algorithm is one type of convolutional neural network (CNN) used for object detection in images. CNNs employ neural networks for deep learning and allow unsupervised feature generation. This algorithm takes an image as input, which it then divides into smaller rectangular regions. From then on, it considers each region to be a separate image. Next, these regions are passed to the CNN, which provides classes and bounding boxes for detected objects. In the case of kidney segmentation, the classes are ”Glomeruli” or ”Non-Glomeruli”. After this is complete for all regions, they are combined to make the original image with glomeruli detected in rectangular boxes. The algorithm outputs the data describing these detection boxes as separate rows in a .csv file which describes each annotation prediction as a single row of data.
![Faster R-CNN Diagram](https://github.com/cns-iu/ccf-research-ftu/blob/master/images/FasterRCNNblockdiagram.png)

## Workflow
![Faster R-CNN Pipeline](https://github.com/cns-iu/ccf-research-ftu/blob/master/images/pipeline%20images/Faster%20RCNN%20Pipeline.jpg)
### Getting Started

First, we set up a directory to work out of, such as C:/Users/*your-username*/Tensorflow, then used the command `git clone https://github.com/tensorflow/models` to clone the tensorflow files into this directory. The COCO API and Proto Buff files were extracted and placed in the ".../Tensorflow/model/research/object_detection" folder. 
We ran the ".../object_detection/bin/protoc.exe" file (`object_detection/protos/*.proto --python_out=.`), then opened the jupyter notebook ".../object_detection/object_detection_tutorial.ipynb" to ensure that the model was working and installation was complete.

### Kidney Data Preprocessing and Manual Annotation Data

Manual annotations used to train the Faster R-CNN model were generated by drawing rectangular boxes around glomeruli using [labelImg](https://tzutalin.github.io/labelImg/). WSIs were manually clipped into 1800x1800 pixel tiles, and 20% of the tiles were reserved for testing purposes. The tiles were opened in labelImg, and rectangular bounding boxes were drawn using the "RectBox" tool around objects with the classification label set as "glomeruli". The annotations for each training or testing tile were exported in .xml format, then they were gathered and transformed into .csv format, which documented tile name, "xmin", "xmax", "ymin", and "ymax" for each annotation, using the ["xml_to_csv.py" script](add link). The resulting training and testing .csv files, along with their respective tile images, were used with the ["generate_tfrecord.py" script](add link) to create tensorflow records that could be used to train the model to detect glomeruli and test its ability to do so.
To generate the tensorflow records, we used the following commands from the "object_detection" folder: 

`python generate_tfrecord.py --csv_input=images\train_labels.csv --image_dir=images\train --output_path=train.record`

`python generate_tfrecord.py --csv_input=images\test_labels.csv --image_dir=images\test --output_path=test.record`

![Faster R-CNN training Images](https://github.com/cns-iu/ccf-research-ftu/blob/master/images/FasterRCNNmanualannotation.png)
### Training

Transfer learning was utilized by starting the Faster R-CNN model with COCO dataset weights. The [pets dataset configuration file](https://github.com/cns-iu/ccf-research-ftu/blob/master/Faster%20R-CNN/faster_rcnn_inception_v2_pets.config), which includes the frozen inference graph for transfer learning, and ["labelmap.pbtxt" file](https://github.com/cns-iu/ccf-research-ftu/blob/master/Faster%20R-CNN/labelmap.pbtxt), which contains the custom label map for glomeruli, were used to begin training the model.
To do this, we first created the folder ".../object_detection/training' with the files "labelmap.pbtxt" and "faster_rcnn_inception_v2_pets.config". Then, we downloaded the file "faster_rcnn_inception_v2_coco" from https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md and extracted it into the object_detection folder.
The "faster_rcnn_inception_v2_pets.config" file was edited to point to the correct filepaths (see image below for example).
![config file screenshot](https://github.com/cns-iu/ccf-research-ftu/blob/master/images/config%20file%20screenshot.png)
".../Tensorflow/models/research" and ".../Tensorflow/models/research/slim" were added to PYTHONPATH.
The following command was run from the object_detection folder:

`python model_main.py --logtostderr --model_dir=training/ --pipeline_config_path=training/faster_rcnn_inception_v2_pets.config`

This initialized the training process, was monitored via Tensorboard using the following command from the object_detection folder:
`tensorboard --logdir=training`

Once the training loss was acceptable, the process was stopped using "Ctrl+C". In our experience, this took 2425 epochs. From the same window, the resulting inference graph was exported using the "export_inference_graph.py" script:
 
`python export_inference_graph.py --input_type image_tensor --pipeline_config_path training/faster_rcnn_inception_v2_pets.config --trained_checkpoint_prefix training/model.ckpt-2425 --output_directory inference_graph`

*Note: The "2425" could be different in other cases. It refers to the final checkpoint in the training process.*

### Segmentation

The ["object_detection_Glomeruli.ipynb" Jupyter notebook](https://github.com/cns-iu/ccf-research-ftu/blob/master/Faster%20R-CNN/object_detection_Glomeruli.ipynb) was utilized for making predictions with the Faster R-CNN Glomeruli Detection model, as well as calculating the accuracy metric Intersect over Union (IoU).
We placed it in the object_detection folder and changed the `MODEL_NAME='inference graph'`, `PATH_TO_FROZEN_GRAPH=MODEL_NAME+'/frozen_inference_graph.pb'`, and `PATH_TO_LABELS='training/labelmap.pbtxt'` fields to reflect our newly-trained model.
The path to the desired image tile to be tested was updated in the notebook, and the model was applied to provide segmentation of the image.
Once it ran successfully, the path to a directory of test images was provided, and the resulting segmentation was compared to the ground truth.

## Results

The Faster R-CNN Glomeruli Detection model we trained was able to detect glomeruli in image tiles with an average precision of 0.941 and average recall of 0.697.