# Mask R-CNN Documentation

## Requirements

* jupyter
* [QuPath](https://qupath.github.io/) or [ImageJ](https://imagej.nih.gov/ij/download.html)
* 

## Data

### Kidney Raw Data

This work utilized a dataset of human kidney whole slide images (WSIs) provided by the HuBMAP Consortium, which is now available at [the HuBMAP Data Portal](https://portal.hubmapconsortium.org/search?origin_sample.mapped_organ[0]=Kidney%20%28Left%29&origin_sample.mapped_organ[1]=Kidney%20%28Right%29&group_name[0]=Vanderbilt%20TMC&entity_type[0]=Dataset). The raw kidney image data was generated by the BIOmolecular Multimodal Imaging Center (BIOMIC) at Vanderbilt University. (See [BIOMIC's page](https://medschool.vanderbilt.edu/biomic/) for more information.) This dataset is composed of 13 samples, each with multiple images that correspond to different microscopy types. All samples include PAS stain images, and many include imaging mass spectrometry as well. In addition, this dataset provides information on where the sample was positioned within the kidney and whether it was taken from a left or right kidney, which is vital information for the HuBMAP objective.

[Kidney Raw Data](https://drive.google.com/drive/folders/14aLxPR9LlzdWXPomAX1moqL0UnRm_RbW?usp=sharing)

## Algorithm

The Mask RCNN algorithm is built upon the Faster RCNN algorithm, but it employs an instance segmentation extension that allows prediction of segmentation masks for each annotation. Rather than relying on the rectangular regions of the Faster RCNN algorithm for outputting detection boxes, the Mask RCNN provides a classification of ”Glomeruli” or ”Non-Glomeruli” to each pixel in the original image. This allows the resulting annotations to be any shape describable by pixels and enables the creation of binary mask overlays for use on the original image.
![Mask R-CNN Diagram](https://github.com/cns-iu/ccf-research-ftu/blob/master/images/MaskRCNNdiagram.jpg)
In our implementation, we utilized transfer learning by initializing the weights from the COCO dataset.

## Workflow
![Mask R-CNN Pipeline](https://github.com/cns-iu/ccf-research-ftu/blob/master/images/pipeline%20images/Mask%20RCNN%20Pipeline.jpg)
### Getting Started

### Kidney Data Preprocessing and Manual Annotation Data

To make the WSIs more manageable, they were split into tiles using the jupyter notebook [Segmentation.ipynb](https://github.com/cns-iu/ccf-research-ftu/blob/master/Mask%20R-CNN/Segmentation.ipynb). This notebook allows for tile height and width to be set as needed, but it is currently set for 800x600 pixel tiles.
From the 13 raw data WSIs, a subset of six PAS stain images were used to generate the ML training dataset. This glomeruli detection training dataset of manually generated glomeruli annotations was created by four users who marked areas corresponding to glomeruli on the six images. All users performed their annotations on laptops that included Windows 10 OS, a trackpad, and 1920x1080 screen resolution. Half of the users made annotations with the software QuPath and its brush tool, while the other half utilized ImageJ with its oval tool. 
The QuPath annotations, which are automatically saved in .qpdata format, were translated into JSON format for use as training data. ImageJ annotations were automatically exported in JSON format. All four users' annotations were combined to find the ground truth, which was determined to be the area of a WSI that all 4 users marked as "glomeruli".

[Manual Glomeruli Annotation Data - Accessible to IU Users Only](https://drive.google.com/drive/folders/1YdOvkIWyWBOc-zSxClC1kVwST8YxVKXc?usp=sharing)

### Training

To train the Mask R-CNN model, the jupyter notebook [MaskRCNN Glomeruli - Training.ipynb](https://github.com/cns-iu/ccf-research-ftu/blob/master/Mask%20R-CNN/MaskRCNN%20Glomeruli%20-%20Training.ipynb) was utilized. It uses the JSON training data files to extract masks of glomeruli on the tile images, then trains the model for 5 epochs to perform glomeruli detection. 

### Segmentation

Once training was complete, the jupyter notebook [MaskRCNN Glomeruli - Detection and Saving.ipynb](https://github.com/cns-iu/ccf-research-ftu/blob/master/Mask%20R-CNN/MaskRCNN%20Glomeruli%20-%20Detection%20and%20Saving.ipynb) was used to detect glomeruli in the image tiles and save the binary mask output in the desired format. The tiles and masks of detected glomeruli were stitched back into WSIs using [Combining images.ipynb](https://github.com/cns-iu/ccf-research-ftu/blob/master/Mask%20R-CNN/Combining%20images.ipynb), and accuracy metrics including Intersection over Union (IoU), precision, and recall were calculated using [Accuracy Metrics.ipynb](https://github.com/cns-iu/ccf-research-ftu/blob/master/Mask%20R-CNN/Accuracy%20Metrics.ipynb) and the ground truth. 

## Results

Our trained Mask R-CNN Glomeruli Detection model exhibited an IoU value of 0.604, precision of 0.612, and recall of 0.959.